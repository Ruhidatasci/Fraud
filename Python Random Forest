import pandas as pd
import numpy as np
import seaborn as sns
# Reading the Fraud Check Data #################
FraudC = pd.read_csv("E:\\Data Science Material\\Decision Tree Assign\\Fraud_check.csv")
FraudC.head()
FraudC
FraudC.columns
FraudC.isnull().sum()
sns.pairplot(FraudC)
FraudC["TaxInc"] = pd.cut(FraudC["Taxable.Income"], bins = [10002,30000,99620], labels = ["Risky", "Good"])
FraudCheck = FraudC.drop(columns=["Taxable.Income"])
FCheck = pd.get_dummies(FraudCheck.drop(columns = ["TaxInc"]))
FraudC_final = pd.concat([FCheck, FraudCheck["TaxInc"]], axis = 1)
colnames = list(FraudC_final.columns)
colnames
predictors = colnames[:9]
predictors
target = colnames[9]
target

X = FraudC_final[predictors]
X.shape
Y = FraudC_final[target]

###################################################Decision Tree Model###################################
from sklearn.model_selection import train_test_split
train, test = train_test_split(FraudC_final, test_size = 0.3)
FraudC_final["TaxInc"].unique()
from sklearn.tree import DecisionTreeClassifier
help(DecisionTreeClassifier)
modelTree = DecisionTreeClassifier(criterion = "entropy")
modelTree.fit(train[predictors], train[[target]])
type([target])
type(predictors)
"""Prediction"""
preds = modelTree.predict(test[predictors])
preds
type(preds)

pd.Series(preds).value_counts()
141/(141+39)
pd.crosstab(test[target],preds) #64%
temp = pd.Series(modelTree.predict(train[predictors])).reset_index(drop=True)
np.mean(pd.Series(train.TaxInc).reset_index(drop=True)==pd.Series(modelTree.predict(train[predictors])))
np.mean(preds==test.TaxInc)
#68%

#######################################Random Forest Model############################################

from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(n_jobs = 3, oob_score = True, n_estimators = 15, criterion = "entropy")
#rf = RandomForestClassifier(n_jobs=3,oob_score=True,n_estimators=15,criterion="entropy")
# n_estimators -> Number of trees ( you can increase for better accuracy)
# n_jobs -> Parallelization of the computing and signifies the number of jobs 
# running parallel for both fit and predict
# oob_score = True means model has done out of box sampling to make predictions

np.shape(FraudC_final) # 600,100 => Shape 
len(Y)
len(X)
FraudC_final.describe()
FraudC_final.info()
type([X])
type([Y])
Y1 = pd.DataFrame(Y)
type(Y1)
#### Attributes that comes along with RandomForest function
rf.fit(X,Y1) # Fitting RandomForestClassifier model from sklearn.ensemble 
rf.estimators_ # 
rf.classes_ # class labels (output)
rf.n_classes_ # Number of levels in class labels 
rf.n_features_  # Number of input features in model 8 here.

rf.n_outputs_ # Number of outputs when fit performed

rf.oob_score_  # 0.7
rf.predict(X)
##############################

FraudC_final['rf_pred'] = rf.predict(X)
cols = ['rf_pred','TaxInc']
FraudC_final[cols].head()
FraudC_final["TaxInc"]

from sklearn.metrics import confusion_matrix

confusion_matrix(FraudC_final['TaxInc'],FraudC_final['rf_pred']) # Confusion matrix

pd.crosstab(FraudC_final['TaxInc'],FraudC_final['rf_pred'])



print("Accuracy",(476+115)/(476+115+9+0)*100)
#98.5%

FraudC_final["rf_pred"]
